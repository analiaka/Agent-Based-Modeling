{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cbd89d-5178-48ff-9ee2-8e3cdfefdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from math import pi\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d8e1b-0090-48dc-8dcf-4098345f90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "# Set the working directory to the path where your files are located\n",
    "os.chdir('path_to_your_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbe127-6d0b-40f1-82c5-4160f2adbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('path_to_your_data/DATA.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40ce74-f104-4cc0-8331-403e5e883f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check your data\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffed13a-d41f-4216-8df1-a35fc461d47e",
   "metadata": {},
   "source": [
    "**PCA Analysis next**\n",
    "\n",
    "\n",
    "Before performing Principal Component Analysis (PCA):\n",
    "\n",
    "- It is important to ensure that your data is clean\n",
    "  \n",
    "  This may include: Excluding non-numeric identifiers, such as the ID column.\n",
    "  \n",
    "- It is crucial to standardize the numeric data to ensure all variables are on a similar scale. PCA is sensitive to the magnitude of the variables, meaning that features with larger ranges could dominate the analysis and distort the results. Standardization transforms the data so that each variable has a mean of 0 and a standard deviation of 1, ensuring that all variables contribute equally to the analysis.\n",
    "\n",
    "In the steps below, we use the StandardScaler from the sklearn library to standardize the numeric data. The process involves:\n",
    "\n",
    "- Applying the scaler to transform the numeric dataset into a standardized format suitable for PCA.\n",
    "This ensures that the subsequent PCA will be based purely on the relative relationships among variables, rather than being influenced by differing scales or units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c2be8-9f60-4714-bd35-8f22d791049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from factor_analyzer import FactorAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ab4ad-cdb8-4b24-ad27-1a5123fd1183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check which columns have any NaN values\n",
    "print(data.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a9b08-9a76-45e1-9a3c-52585060486f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for any remaining NaNs\n",
    "print(data.isna().sum())\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "print(data.dtypes)\n",
    "\n",
    "# Convert all columns to numeric if necessary\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Re-check if any NaN values remain\n",
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa90975-a6d0-4f24-ba33-405d3ec80f87",
   "metadata": {},
   "source": [
    "You may need to exclude the first column in the analysis because of the data format. If you need to do so, the following are some options to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5faed3-a902-4e3c-a01e-0366ffd48bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the first column (e.g., ID column) and use it as the index if needed. Choose one option below and comment out the other one \n",
    "data.set_index(data.columns[0], inplace=True)  # Optional: set the first column as the index\n",
    "data_features = data.iloc[:, 1:]  # Select all columns except the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a64cc-16f3-442d-a45e-9fdc14efe731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numeric data\n",
    "data2 = data.iloc[:, 1:]  # Exclude the 'ID' column\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d2eff-f954-4d69-bd43-76e41eb9c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA - test out how different numbers of components impact your results\n",
    "# The step below includes a step to rotate using varimax\n",
    "pca = PCA(n_components=10)  # Adjust components as needed (start with 10 then change it to see the result)\n",
    "pca_result = pca.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd03195-3777-4afc-ba80-9c3be9755ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fit the PCA model with rotation\n",
    "fa = FactorAnalyzer(rotation='varimax', n_factors=10)\n",
    "fa.fit(data_scaled)\n",
    "\n",
    "# Get the rotated loadings\n",
    "rotated_loadings = fa.loadings_\n",
    "\n",
    "# Create a DataFrame for the rotated loadings\n",
    "rotated_loadings_df = pd.DataFrame(\n",
    "    rotated_loadings,\n",
    "    index=data2.columns,\n",
    "    columns=[f'Rotated PC{i+1}' for i in range(rotated_loadings.shape[1])]\n",
    ")\n",
    "\n",
    "# Save the rotated loadings to CSV\n",
    "rotated_loadings_df.to_csv('rotated_pca_loadings.csv', index=True)\n",
    "\n",
    "# Display the rotated loadings\n",
    "print(rotated_loadings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135ec48-d294-4a5c-9b9b-00aa885066fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get eigenvalues (equivalent to explained variance)\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "# Get explained variance and cumulative explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variance.cumsum()\n",
    "\n",
    "#  Create a DataFrame to display the results\n",
    "variance_df = pd.DataFrame({\n",
    "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance))],\n",
    "    'Eigenvalue': eigenvalues,\n",
    "    'Explained Variance': explained_variance,\n",
    "    'Cumulative Explained Variance': cumulative_explained_variance\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(variance_df)\n",
    "\n",
    "# Save to CSV if needed\n",
    "variance_df.to_csv('pca_variance_explained.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94483539-243c-4574-a25b-49b9a0419da2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Scree Plot**\n",
    "\n",
    "\n",
    "The Scree Plot is a key visualization tool in Principal Component Analysis (PCA) that helps determine the number of principal components to retain for meaningful analysis. It plots the eigenvalues of the components in descending order against their corresponding principal component numbers. Eigenvalues represent the amount of variance explained by each principal component, and the plot helps identify where the explained variance begins to level off, often referred to as the \"elbow point.\" This point indicates the optimal number of components to retain, balancing dimensionality reduction with the preservation of data variance. In this section, we create a Scree Plot to guide the selection of principal components for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536f46c-68e6-4513-acf1-b4b7e28365c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039bab4-5cca-4253-b14a-d47058b4ca5e",
   "metadata": {},
   "source": [
    "**Cumulative Explained Variance Plot**\n",
    "\n",
    "\n",
    "The Cumulative Explained Variance Plot is an essential tool in Principal Component Analysis (PCA) for understanding how much of the total variance in the data is captured by the principal components. It displays the cumulative proportion of variance explained as the number of components increases. This plot helps identify the minimum number of components required to capture a desired level of variance (e.g., 90% or 95%). By examining this plot, we can make informed decisions about the dimensionality reduction, ensuring that the selected components retain the majority of the information from the original dataset while reducing complexity. In this section, we calculate and visualize the cumulative explained variance to guide the selection of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984d974-92ed-43b7-8cc0-0e76c760af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance Plot')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb8e4a-727f-4377-b656-dad7a89d2a2c",
   "metadata": {},
   "source": [
    "**The Biplot**\n",
    "\n",
    "The Biplot is a powerful visualization in Principal Component Analysis (PCA) that combines information about both the observations and the variables. It provides insights into the relationships among variables, the distribution of observations, and how the variables contribute to the principal components.\n",
    "\n",
    "In this biplot:\n",
    "\n",
    "- The scores (dots) represent the projections of the observations onto the first two principal components (PC1 and PC2).\n",
    "- The loadings (arrows) show the contributions of the original variables to these principal components. The direction and length of the arrows indicate the variable's influence and its relationship with the components.\n",
    "  \n",
    "This visualization helps interpret the principal components by linking them back to the original variables, making it easier to understand how the reduced dimensions capture the variability in the dataset. In this section, we construct a biplot for the first two principal components to explore these relationships visually. You can plot different components to get an understanding of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82e18b-9a86-41a0-8304-470038cbea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Extract PC scores (first two principal components)\n",
    "pc_scores = pca.fit_transform(data_scaled)[:, :2]\n",
    "\n",
    "# Step 2: Extract loadings (first two components)\n",
    "loadings = pca.components_.T[:, :2]\n",
    "\n",
    "# Step 3: Create a biplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(pc_scores[:, 0], pc_scores[:, 1], alpha=0.6, color='gray', edgecolor='k', label='Observations')\n",
    "\n",
    "# Plot the loadings (arrows) and variable names\n",
    "for i, (x, y) in enumerate(loadings):\n",
    "    plt.arrow(0, 0, x, y, color='red', alpha=0.7)\n",
    "    plt.text(x * 1.15, y * 1.15, data2.columns[i], color='red', ha='center', va='center')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Biplot of the First Two Principal Components')\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca9aab-2803-43ca-afd1-e017425794ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Adjust the scaling factor for the loadings\n",
    "scaling_factor = 3  # Increase to separate the arrows\n",
    "\n",
    "# Step 1: Extract PC scores (first two principal components)\n",
    "pc_scores = pca.fit_transform(data_scaled)[:, :2]\n",
    "\n",
    "# Step 2: Extract loadings and scale them\n",
    "loadings = pca.components_.T[:, :2] * scaling_factor\n",
    "\n",
    "# Step 3: Create an improved biplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(pc_scores[:, 0], pc_scores[:, 1], alpha=0.6, color='gray', edgecolor='k', label='Observations')\n",
    "\n",
    "# Plot the scaled loadings (arrows) and variable names\n",
    "for i, (x, y) in enumerate(loadings):\n",
    "    if np.abs(x) > 0.3 or np.abs(y) > 0.3:  # Display only variables with high loadings\n",
    "        plt.arrow(0, 0, x, y, color='red', alpha=0.7)\n",
    "        plt.text(x * 1.15, y * 1.15, data2.columns[i], color='red', ha='center', va='center')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Improved Biplot of the First Two Principal Components')\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66eef6-a437-40c4-bf1c-8f02cab168dd",
   "metadata": {},
   "source": [
    "**Principal Component Score Plot**\n",
    "\n",
    "The Principal Component (PC) Score Plot is a visualization that displays the distribution of observations in the reduced dimensional space defined by the first two principal components (PC1 and PC2). Each point in the plot represents an observation, with its coordinates determined by the scores for PC1 and PC2.\n",
    "\n",
    "This plot provides insights into:\n",
    "\n",
    "- Clustering: Identifying groups or patterns among observations.\n",
    "- Outliers: Detecting observations that are distinctly separated from others.\n",
    "- Relationships: Exploring the spread and orientation of observations along the principal components.\n",
    "  \n",
    "In this section, we create a scatter plot of the first two principal components to analyze the distribution and relationships among the observations in the reduced space. This visualization serves as a foundation for interpreting the PCA results and understanding the structure of the data. Plot different components as needed to better understand your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b18fe8-a33c-49ea-a95b-2b034c4b646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the PC scores (first two components)\n",
    "pc1 = pc_scores[:, 0]\n",
    "pc2 = pc_scores[:, 1]\n",
    "\n",
    "# Step 2: Create a scatter plot of the first two PCs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pc1, pc2, color='blue', alpha=0.6, edgecolor='k')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PC Score Plot: PC1 vs PC2')\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7fe8dd-004c-4479-b6ff-5d5c7560156e",
   "metadata": {},
   "source": [
    "All the previous steps—data preparation, PCA, visualization of principal components, and exploration of relationships among variables—were designed to deepen our understanding of the dataset's structure and uncover potential patterns. These analyses provided valuable insights into the variance in the data, the relationships among variables, and the distribution of observations in the reduced dimensional space. By identifying patterns, clusters, or groupings within the data, we are now better equipped to perform a focused and meaningful cluster analysis. The next step will leverage these findings to group similar observations into clusters, facilitating a more detailed interpretation of underlying patterns and enabling data-driven decision-making.\n",
    "\n",
    "**NEXT IS THE CLUSTER ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571d962-1902-4e20-8e23-4d3049b04857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data needs to be structured:\n",
    "# Columns that I am using at this initial stage are those related to the extended TPB THAT HAVE BEEN IDENTIFIED BASED ON THE PCA.\n",
    "\n",
    "# Consolidation of each construct by averaging its related columns is a vital fist step\n",
    "# Then column names are adjusted to match data structure\n",
    "data2['Behavior'] = data2[['Behavior1 Coded', 'Behavior2 Coded', 'Behavior3 Coded',\t'Behavior4 Coded', 'Behavior5 Coded', 'Behavior6 Coded', 'Behavior7 Coded', 'Behavior8 Coded', 'Behavior9 Coded']].mean(axis=1)\n",
    "data2['Attitude'] = data2[['Attitude3 Coded', 'Attitude4 Coded', 'Attitude5 Coded', 'Attitude6 Coded', 'Attitude7 Coded', 'Attitude8 Coded', 'Attitude9 Coded']].mean(axis=1)\n",
    "data2['Social_Norms'] = data2[['SNorms2 Coded', 'SNorms3 Coded']].mean(axis=1)\n",
    "data2['PBC'] = data2[['PBC1 Coded', 'PBC2 Coded',\t'PBC3 Coded', 'PBC4 Coded']].mean(axis=1)\n",
    "data2['Moral_Norms'] = data2[['Moral1 Coded', 'Moral2 Coded', 'Moral3 Coded', 'Moral4 Coded']].mean(axis=1)\n",
    "data2['Perceived_Risk'] = data2[['RiskP2 Coded', 'RiskP3 Coded', 'RiskP4 Coded', 'RiskP5 Coded', 'RiskP6 Coded']].mean(axis=1)\n",
    "data2['Trust'] = data2[['Trust2 Coded', 'Trust3 Coded']].mean(axis=1)\n",
    "data2['Climate_Perc'] = data2[['CCPerception1 Coded', 'CCPerception2 Coded', 'CCPerception4 Coded', 'CCPerception5 Coded', 'CCPerception6 Coded', 'CCPerception7 Coded', 'CCPerception8 Coded', 'CCPerception9 Coded', 'CCPerception11 Coded', 'CCPerception12 Coded', 'CCPerception13 Coded']].mean(axis=1)\n",
    "\n",
    "# Optional: Drop the original individual columns if they are no longer needed and create a new one\n",
    "#data2 = data[['Behavior', 'Attitude', 'Social_Norms', 'PBC', 'Moral_Norms', 'Perceived_Risk', 'Trust', 'Climate_Perc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f023a4d-5e9d-4466-9551-f8d0692e5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e73198-a24d-4e57-a371-9178bfa44128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('survey with structured data2.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed093b9-0e4d-4eae-b31e-775ae112f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining NaNs\n",
    "print(data2.isna().sum())\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "print(data2.dtypes)\n",
    "\n",
    "# Convert all columns to numeric if necessary\n",
    "data = data2.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Re-check if any NaN values remain\n",
    "print(data2.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff2869-a462-400a-ac4f-e7c44e6a72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with any NaN values\n",
    "data2 = data2.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb1731-b3e7-45b7-993a-9147d4b5a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next clustering and visualization \n",
    "# Sample DataFrame with standardized TPB variables\n",
    "# Replace this with your actual data\n",
    "\n",
    "# Step 1: Clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "data2['cluster'] = kmeans.fit_predict(data2)\n",
    "\n",
    "# Step 2: Calculate mean values for each cluster using the new data\n",
    "cluster_means = data2.groupby('cluster').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ea964-6844-48c9-9317-7e6f679ab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Verify the new column\n",
    "print(data2.head())  # Check the first few rows to confirm\n",
    "print(data2['cluster'].value_counts())  # Check the number of observations in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cda70-ef11-4f10-a820-d0ee7f2b29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check cluster assignments\n",
    "print(data2['cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e9eb8-0dd9-4f93-aff5-5649177b68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('survey with structured data and clusters.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7191b7-dacb-4b2c-a53d-143283f16037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Radar Chart Setup\n",
    "categories = ['Attitude', 'Social_Norms', 'PBC', 'Moral_Norms', 'Perceived_Risk', 'Trust', 'Climate_Perc']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Create radar chart for each cluster\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "fig.suptitle(\"Radar Charts for Each Cluster\")\n",
    "\n",
    "# Define the angles for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Calculate the maximum value across all categories and clusters\n",
    "max_value = cluster_means[categories].max().max()\n",
    "\n",
    "for i in range(4):\n",
    "    # Get the mean values for the relevant categories only\n",
    "    values = cluster_means.loc[i, categories].values.flatten().tolist()\n",
    "    values += values[:1]  # Repeat the first value to close the circle\n",
    "\n",
    "    # Create subplot for each cluster\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axis per variable and add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "\n",
    "    # Set consistent y-axis scale for all charts\n",
    "    ax.set_ylim(0, max_value)\n",
    "    ax.set_yticks([1, 2, 3, 4, max_value])\n",
    "\n",
    "    # Plot data and fill area\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid')\n",
    "    ax.fill(angles, values, alpha=0.4)\n",
    "\n",
    "    # Title for each subplot\n",
    "    ax.set_title(f\"Cluster {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04dfe4-39dd-4ad2-8ad0-0d0ff8ef4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# Define categories and labels\n",
    "categories = ['Attitude', 'Social_Norms', 'PBC', 'Moral_Norms', 'Perceived_Risk', 'Trust', 'Climate_Perc']\n",
    "cluster_labels = ['Risk-Averse Adopters', 'Socially Influenced Adopters', 'Innovative Adopters', 'Traditionalists']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Define the angles for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Set a fixed maximum value for all radar charts\n",
    "max_value = 5\n",
    "\n",
    "# Create radar charts for each cluster\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12), subplot_kw=dict(polar=True))\n",
    "fig.suptitle(\"Radar Charts for Each Cluster (Standardized Scale)\")\n",
    "\n",
    "for i in range(4):\n",
    "    # Get the mean values for the relevant categories only\n",
    "    values = cluster_means.loc[i, categories].values.flatten().tolist()\n",
    "    values += values[:1]  # Repeat the first value to close the circle\n",
    "\n",
    "    # Create subplot for each cluster\n",
    "    ax = axs[i // 2, i % 2]\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axis per variable and add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "\n",
    "    # Set consistent y-axis scale for all charts\n",
    "    ax.set_ylim(0, max_value)\n",
    "    ax.set_yticks([0, 1, 2, 3, 4, max_value])\n",
    "\n",
    "    # Plot data and fill area\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid')\n",
    "    ax.fill(angles, values, alpha=0.4)\n",
    "\n",
    "    # Title for each subplot with cluster labels\n",
    "    ax.set_title(f\"Cluster {i+1}: {cluster_labels[i]}\")\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac4dce-6527-40c9-b978-08e4917d84d1",
   "metadata": {},
   "source": [
    "**TRYING SANKEY DIAGRAM**\n",
    "\n",
    "\n",
    "A Sankey Diagram is a powerful visualization tool used to depict flows or connections between different categories or variables. The width of each flow is proportional to its value, making it easy to understand the relative importance or magnitude of connections. Sankey diagrams are particularly useful for visualizing relationships, such as transitions, distributions, or resource flows, across multiple stages or categories. In this section, we will create a Sankey diagram to represent the relationships in our data (demographic variables to our clusters), using the appropriate Python libraries (e.g., plotly or matplotlib). By constructing this diagram, we can gain valuable insights into how different categories interact, helping to identify key pathways or dominant flows within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64092206-6aa0-43a5-b53a-66f4bda028fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf9760-cb42-4585-9a68-c980daef740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv(\"path-to-your-data/SANKEY.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7d618-4311-4a33-b120-f409392bf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for labels\n",
    "age_map = {0: 'Young', 1: 'Middle-aged', 2: 'Senior'}\n",
    "education_map = {\n",
    "    0: 'Prefer not to say',\n",
    "    1: '12th Grade or Less',\n",
    "    2: 'High School Diploma',\n",
    "    3: 'Trade/Vocational',\n",
    "    4: 'Bachelor’s Degree',\n",
    "    5: 'Post Graduate'\n",
    "}\n",
    "political_map = {\n",
    "    0: 'Prefer not to say',\n",
    "    1: 'Democrat',\n",
    "    2: 'Republican',\n",
    "    3: 'Independent'\n",
    "}\n",
    "cluster_map = {\n",
    "    0: 'Risk-Averse',\n",
    "    1: 'Socially Influenced',\n",
    "    2: 'Conditional',\n",
    "    3: 'Traditionalists'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6edc5-a04b-49b6-ba59-e7893923b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize Age into groups\n",
    "data['Age Group'] = pd.cut(data['Age'], bins=[0, 35, 60, 100], labels=['Young', 'Middle-aged', 'Senior'])\n",
    "\n",
    "# Group the data based on Age, Education, and Political Affiliation with clusters\n",
    "age_flow = data.groupby(['Age Group', 'cluster']).size().reset_index(name='count')\n",
    "education_flow = data.groupby(['Highest level of education', 'cluster']).size().reset_index(name='count')\n",
    "political_flow = data.groupby(['Political affiliation', 'cluster']).size().reset_index(name='count')\n",
    "\n",
    "# Define labels for Sankey diagram using mappings\n",
    "labels = (list(age_map.values()) +\n",
    "          list(education_map.values()) +\n",
    "          list(political_map.values()) +\n",
    "          list(cluster_map.values()))\n",
    "\n",
    "# Create a label mapping\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Initialize lists for sources, targets, and values\n",
    "sources = []\n",
    "targets = []\n",
    "values = []\n",
    "\n",
    "# Age Group to Clusters\n",
    "for _, row in age_flow.iterrows():\n",
    "    sources.append(label_map[row['Age Group']])\n",
    "    targets.append(label_map[cluster_map[row['cluster']]])\n",
    "    values.append(row['count'])\n",
    "\n",
    "# Education Level to Clusters\n",
    "for _, row in education_flow.iterrows():\n",
    "    sources.append(label_map[education_map[row['Highest level of education']]])\n",
    "    targets.append(label_map[cluster_map[row['cluster']]])\n",
    "    values.append(row['count'])\n",
    "\n",
    "# Political Affiliation to Clusters\n",
    "for _, row in political_flow.iterrows():\n",
    "    sources.append(label_map[political_map[row['Political affiliation']]])\n",
    "    targets.append(label_map[cluster_map[row['cluster']]])\n",
    "    values.append(row['count'])\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=labels,\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values\n",
    "    )\n",
    "))\n",
    "\n",
    "# Update layout and show the diagram\n",
    "fig.update_layout(title_text=\"Sankey Diagram of Farmer Clusters by Age, Education, and Political Affiliation\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da813cc-eac7-411a-8c51-53dfc55277f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize Age into groups\n",
    "# Ensure Age Group column is created correctly\n",
    "data['Age Group'] = pd.cut(data['Age'], bins=[0, 35, 60, 100], labels=[0, 1, 2])  # Use numeric labels (0, 1, 2)\n",
    "age_map = {0: 'Young', 1: 'Middle-aged', 2: 'Senior'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a934cb6-0d86-44fd-873c-6879c6738629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create separate Sankey diagrams for each variable\n",
    "\n",
    "def create_sankey(source_column, source_map, title):\n",
    "    # Group the data\n",
    "    flow_data = data.groupby([source_column, 'cluster']).size().reset_index(name='count')\n",
    "    \n",
    "    # Define labels\n",
    "    labels = list(source_map.values()) + list(cluster_map.values())\n",
    "    label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "    # Initialize lists for sources, targets, and values\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "\n",
    "    # Create flows\n",
    "    for _, row in flow_data.iterrows():\n",
    "        sources.append(label_map[source_map[row[source_column]]])\n",
    "        targets.append(label_map[cluster_map[row['cluster']]])\n",
    "        values.append(row['count'])\n",
    "\n",
    "    # Create the Sankey diagram\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=labels,\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    # Update layout and show the diagram\n",
    "    fig.update_layout(title_text=title, font_size=10)\n",
    "    fig.show()\n",
    "\n",
    "# Sankey Diagram for Age\n",
    "create_sankey('Age Group', age_map, \"Sankey Diagram: Age vs. Farmer Clusters\")\n",
    "\n",
    "# Sankey Diagram for Political Affiliation\n",
    "create_sankey('Political affiliation', political_map, \"Sankey Diagram: Political Affiliation vs. Farmer Clusters\")\n",
    "\n",
    "# Sankey Diagram for Education\n",
    "create_sankey('Highest level of education', education_map, \"Sankey Diagram: Education vs. Farmer Clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d206d-9b00-482a-becc-ac455e1fd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group the data based on Age, Education, and Political Affiliation with clusters\n",
    "age_flow = data.groupby(['Age Group', 'cluster']).size().reset_index(name='count')\n",
    "education_flow = data.groupby(['Highest level of education', 'cluster']).size().reset_index(name='count')\n",
    "political_flow = data.groupby(['Political affiliation', 'cluster']).size().reset_index(name='count')\n",
    "\n",
    "# Define source, target, and value lists for the Sankey diagram using coded values\n",
    "sources = []\n",
    "targets = []\n",
    "values = []\n",
    "\n",
    "# Use coded values directly for sources (Age: 1 = Young, 2 = Middle-aged, 3 = Senior)\n",
    "# Age Group to Clusters\n",
    "for _, row in age_flow.iterrows():\n",
    "    sources.append(row['Age Group'])  # Use Age Group coded values\n",
    "    targets.append(row['cluster'] + 10)  # Offset target indices for clusters to avoid overlap\n",
    "    values.append(row['count'])\n",
    "\n",
    "# Education Level to Clusters (Coded: 1 to 5 for education levels)\n",
    "for _, row in education_flow.iterrows():\n",
    "    sources.append(row['Highest level of education'])\n",
    "    targets.append(row['cluster'] + 10)  # Offset for clusters\n",
    "    values.append(row['count'])\n",
    "\n",
    "# Political Affiliation to Clusters (Coded: 1 = Democrat, 2 = Republican, 3 = Independent)\n",
    "for _, row in political_flow.iterrows():\n",
    "    sources.append(row['Political affiliation'])\n",
    "    targets.append(row['cluster'] + 10)  # Offset for clusters\n",
    "    values.append(row['count'])\n",
    "\n",
    "# Create the Sankey diagram without explicit labels\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        # Empty labels list\n",
    "        label=[]\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram Without Explicit Labels\", font_size=10)\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
